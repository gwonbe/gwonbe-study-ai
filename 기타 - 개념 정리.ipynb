{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMX0zMUBjvZRisYzPCQ8jJv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ❇️ 기본 개념\n","\n","#### * 노드, 뉴런\n","- 신경망에서 데이터 처리를 담당하는 기본 단위\n","- 이전 노드에서 값을 받아 계산하고 결과를 다음 노드로 전달\n","\n","#### * 엣지\n","- 노드와 노드를 연결하는 선\n","\n","#### * 레이어\n","- 신경망의 구조에서 데이터가 처리되는 각 단계\n","\n","#### * 은닉층 (hidden layer)\n","- 인공신경망에서 입력층과 출력층 사이에 위치하는 층\n","- 데이터로부터 복잡한 특징이나 패턴을 추출하는 역할을 수행\n","\n","#### * 손실\n","- 모델의 예측값과 실제값 간의 차이\n","- 모델이 얼마나 잘못 예측했는지를 나타내는 값\n","\n","#### * 손실 함수\n","- 손실을 계산하는 함수\n","\n","#### * 활성화 함수 (Activation Function)\n","- 입력 신호의 총합을 출력 신호로 변환하는 함수\n","- 각 노드의 값을 다음으로 전달하기 전, 해당 값을 활성화할지 기준이 되는 함수\n","\n","<br>"],"metadata":{"id":"jfiKq-GSfrCa"}},{"cell_type":"markdown","source":["# ❇️ 과대적합 과소적합\n","\n","#### * 과대적합(overfitting)\n","- 모델이 훈련 세트에 과하게 적합한 상태\n","- 훈련 세트에서는 높은 정확도를 보이나 그 외 세트는 낮은 정확도를 보이는 현상\n","\n","#### * 과소적합(underfitting)\n","- 모델이 훈련 세트의 규칙을 제대로 찾지 못한 상태\n","- 훈련 세트과 그 외의 세트 모두 정확도가 낮은 현상\n","\n","<br>\n","\n","# ❇️ 과대적합 과소적합 - 발생원인\n","\n","#### * 과대적합(overfitting)\n","- 데이터 세트 내 데이터가 충분하지 못한 경우\n","- 데이터 세트 내 분산이 크거나 노이즈가 심한 경우\n","- 모델의 복잡도가 큰 경우\n","- 과도하게 큰 epoch로 학습하는 경우\n","\n","#### * 과대적합(underfitting)\n","- 모델의 복잡도가 낮은 경우\n","- 모델에 너무 많은 규제가 적용된 경우\n","- 충분하지 못한 epoch로 학습하는 경우\n","\n","<br>"],"metadata":{"id":"NZrjJOKjtVAf"}},{"cell_type":"markdown","source":["# ❇️ 레이어의 종류\n","\n","#### * Dense Layer (완전 연결 층)\n","- 입력과 출력이 전부 연결\n","- 신경망 구조의 가장 기본적인 형태\n","- y = f(Wx+b) 수식을 만족 (W는 가중치, b는 편향)\n","- 출력 = 활성화함수 * (가중치행렬*입력+편향)\n","\n","#### * Dropout Layer (드롭아웃 층)\n","- 과적합 방지를 위해 학습 중 일부 뉴런을 임의로 비활성화\n","- 학습 시에만 적용하고 예측할 때와 시험할 때는 미적용\n","\n","#### * Flatten Layer (평탄화 층)\n","- 다차원 행렬을 1차원 벡터로 변환\n","- 주로 이미지 분석 시 CNN 모델에서 마지막 합성곱 층 뒤에 사용\n","\n","#### * Conv1D Layer (1D 합성곱 층)\n","- CNN(Convolutional Neural Networks)에서 사용\n","- 합성곱의 방향은 한 방향 (가로)\n","- 주로 sequence 모델과 자연어 처리 (Natural Language Processing, NLP) 에서 사용\n","\n","#### * Conv2D Layer (2D 합성곱 층)\n","- CNN(Convolutional Neural Networks)에서 사용\n","- 합성곱의 방향은 두 방향 (가로, 세로)\n","- 주로 이미지 처리에서 사용\n","\n","#### * Conv3D Layer (3D 합성곱 층)\n","- 합성곱 신경망 'CNN' (Convolutional Neural Networks)에서 사용\n","- 합성곱의 방향은 세 방향 (가로, 세로, 높이)\n","- 주로 비디오 처리에서 사용\n","\n","#### * MaxPooling1D Layer\n","- CNN에서 특징 맵의 차원을 축소하고, 연산량을 줄여, 중요한 특성을 추출\n","- 부분 영역마다 최대값만 추출\n","- 주로 sequence 모델과 자연어 처리 (Natural Language Processing, NLP) 에서 사용\n","- 입력 데이터 형태: (batch_size, steps, channels) — 여기서 steps는 시퀀스의 길이, channels는 각 시점에서의 특성\n","- 출력 데이터 형태: (batch_size, pooled_steps, channels) — 풀링된 결과는 시퀀스 길이가 축소된 상태로 출력\n","\n","#### * MaxPooling2D Layer\n","- CNN에서 특징 맵의 차원을 축소하고, 연산량을 줄여, 중요한 특성을 추출\n","- 부분 영역마다 최대값만 추출\n","- 주로 이미지 처리에서 사용\n","- 입력 데이터 형태: (batch_size, height, width, channels) — 채널수는 RGB 이미지의 경우 3\n","- 출력 데이터 형태: (batch_size, pooled_height, pooled_width, channels) — 풀링된 이미지의 크기가 축소\n","\n","#### * MaxPooling3D Layer\n","- CNN에서 특징 맵의 차원을 축소하고, 연산량을 줄여, 중요한 특성을 추출\n","- 부분 영역마다 최대값만 추출\n","- 주로 비디오 처리에서 사용\n","- 입력 데이터 형태: (batch_size, depth, height, width, channels) — 여기서 depth는 3D 데이터의 깊이\n","- 출력 데이터 형태: (batch_size, pooled_depth, pooled_height, pooled_width, channels) — 풀링된 3D 데이터의 차원이 축소\n","\n","<br>"],"metadata":{"id":"X2KMjBpnZshy"}},{"cell_type":"markdown","source":["# ❇️ 활성화 함수\n","\n","#### * Softmax\n","- 다중 클래스 분류에서 사용한다.\n","- 각 클래스에 대한 예측 확률을 계산할 때 유용한다.\n","- 클래스가 N개일 때, 각 N개에 대한 확률을 출력값으로 갖는다.\n","- 입력값을 각각 지수함수로 취하고, 이를 정규화해 입력값의 총합을 1로 만든다.\n","- 출력값은 각각 0~1 값을 가지며, 출력값의 총합은 반드시 1이다.\n","\n","#### * Sigmoid (시그모이드 : S자 모양)\n","- 출력값을 0과 1 사이로 제한한다.\n","- 큰 음수 값일수록 0에 가까워지고 큰 양수 값일수록 1에 가까워진다.\n","- 기울기 소실 문제가 발생해 학습 능력이 제한되는 포화 현상이 발생한다.\n","- 그래프 형태가 S자 모양이다.\n","- 주로 확률을 예측할 때 사용한다.\n","\n","#### * Tanh (HyperbolicTangent : 하이퍼볼릭탄젠트)\n","- 출력값이 -1과 1 사이로 제한된다.\n","- 입력값이 0에 가까울수록 미분이 크기 대문에 출력값이 빠르게 변한다.\n","- Sigmoid보다 범위가 넓어 학습이 효과적이지만. 여전히 기울기 소실 문제가 있다.\n","\n","#### * ReLU (Rectified Linear Unit) (렐루) (Rectified : 렉터파이드 : 수정된)\n","- 입력값이 양수이면 그대로 음수이면 0을 출력한다.\n","- 죽은 뉴런을 회생하는데 어려움이 존재한다.\n","- 구현이 단순하고 임계값(양수음수 여부)만 활용해 연산 속도가 빠르다.\n","\n","#### * Leaky ReLU (리키 : 새어나가는)\n","- 입력값이 양수이면 그대로 음수이면 0이 아닌 0.001과 같이 매우 작게 출력한다.\n","- 뉴런이 죽는 현상을 방지할 수 있다.\n","\n"," <br>"],"metadata":{"id":"d6WVmG7ncH3z"}},{"cell_type":"markdown","source":["# ❇️ 정확도와 손실 값의 예시\n","\n","###문제의 특성에 따라 기준은 다를 수 있지만, 일반적인 경우 다음과 같은 수치를 목표로 할 수 있습니다.\n","\n","<br>\n","\n","[분류 문제]\n","- 훈련 정확도: 85% 이상\n","- 검증 정확도: 85% 이상 (훈련 정확도와 비슷하거나 약간 낮을 수 있음)\n","- 훈련 손실: 0.3 이하\n","- 검증 손실: 0.3 이하\n","\n","[회귀 문제]\n","- 훈련 정확도: 해당 문제에 따라 다름 (R² score 등으로 평가)\n","- 검증 정확도: 해당 문제에 따라 다름 (R² score 등으로 평가)\n","- 훈련 손실 (예: MSE): 0.1 이하\n","- 검증 손실 (예: MSE): 0.1 이하\n","\n","<br>\n"],"metadata":{"id":"EhXt9FR9i_ie"}},{"cell_type":"markdown","source":["# ❇️ 인공지능 학습을 위한 최소 데이터 양 조사\n","\n","#### * 통계 분석\n","- 최소 500개 이상\n","\n","#### * 머신러닝 분야\n","- 변수 개수에 100을 곱한 것보다 많은 양\n","\n","#### * 단순한 예측 모델\n","- 몇 백 ~ 몇 천\n","\n","#### * 복잡한 예측 모델\n","- 몇 천 ~ 몇 만\n","\n","#### * 분류 모델\n","- 클래스 당 최소 1,000개 (각 클래스별 샘플 개수가 균형이 맞아야 함)\n","\n","#### * 이미지 분류 모델\n","- 최소 1,000~5,000\n","\n","#### * 딥러닝 기반 이미지 분류 모델\n","- 일반적으로 수만 개 이상\n","\n","<br>\n","\n","### 💟 더불어 테스트용으로 학습 데이터의 최소 10% ~ 20% 만큼 필요\n","\n","<br>"],"metadata":{"id":"7x7YKFq4xVzP"}}]}